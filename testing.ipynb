{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from pprint import pprint\n",
    "from langdetect import detect\n",
    "import unidecode\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_string(text):\n",
    "    return unidecode.unidecode(text)\n",
    "\n",
    "def add_line_numbers(text):\n",
    "    lines = text.split('\\n')\n",
    "    numbered_lines = [f\"{i+1}. {line}\" for i, line in enumerate(lines)]\n",
    "    return '\\n'.join(numbered_lines)\n",
    "\n",
    "def preprocess_pdf(path):\n",
    "    try:\n",
    "        docs = PyPDFLoader(path, mode='single',extraction_mode='layout',pages_delimiter=\"\\n------- PAGE END -------\\n\").load()\n",
    "    except:\n",
    "        docs = PyPDFLoader(path, mode='single',extraction_mode='plain',pages_delimiter=\"\\n------- PAGE END -------\\n\").load()\n",
    "\n",
    "    full_text = docs[0].page_content\n",
    "    full_text = re.sub(r'\\n\\s*\\n', '\\n\\n', full_text)\n",
    "    sanitized_text = sanitize_string(full_text)\n",
    "    return sanitized_text\n",
    "\n",
    "class Article(BaseModel):\n",
    "    title: str = Field(description=\"The title of the article\")\n",
    "    author: str|None = Field(description=\"The author of the article, leave empty if not found\")\n",
    "    summary: str|None = Field(description=\"A short summary of the article. 1 to 5 sentences.\")\n",
    "    published_date: str = Field(description=\"date of publication of the article. DD-MM-YYYY format. use first of the month if exact date isn't found.\")\n",
    "    start_line: int = Field(description=\"The line number of the first line of the article\")\n",
    "    end_line: int = Field(description=\"The line number of the last line of the article\")\n",
    "    start_page: int = Field(description=\"The page number of the first page of the article\")\n",
    "    end_page: int = Field(description=\"The page number of the last page of the article\")\n",
    "\n",
    "class ArticleList(BaseModel):\n",
    "    scratchpad: str = Field(description=\"space for thinking and reasoning about the articles before extraction\")\n",
    "    articles: list[Article]\n",
    "\n",
    "def print_cost(cb):\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Cost: ${cb.prompt_tokens * 3e-6:.6f}\")\n",
    "    print(f\"Completion Cost: ${cb.completion_tokens * 15e-6:.6f}\")\n",
    "    print(f\"Total Cost: ${cb.prompt_tokens * 3e-6 + cb.completion_tokens * 15e-6:.6f}\")\n",
    "\n",
    "def extract_articles(path:str) -> list[Document]:\n",
    "    system_prompt = \"\"\"\n",
    "    ROLE:\n",
    "    You are a PDF parser. You are given text from a PDF file and you need to parse it into articles based on line numbers.\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    - Start and end articles on page ends (marked by ------- PAGE END -------) unless the article starts or ends in the middle of a page.\n",
    "    - ignore pages with only photos + captions. Photos are marked with brackets () and there will be a caption above or below it. Here is an example:\n",
    "        <example>\n",
    "        At Ãdi-Sthala at Vittal - Mrittikã-haran on 2-12-2024.\n",
    "        (Photos of this event by Prashant Haridas and Ashwin Cherka)\n",
    "\n",
    "        Samuhika prãrtana at Srimath Ananteshwara Temple Vittal\n",
    "                            before Dhwajarohana (Kodi).\n",
    "        </example>\n",
    "    - articles are usually 1-15 pages long.\n",
    "    - articles may or may not have authors but there will always be a title.\n",
    "    - The first page is the cover page and it is an article (title should be \"Cover Page\").\n",
    "    - pay attention to the table of contents (this is also an article) and the page numbers in it.\n",
    "    - the published date will most likely be found on the cover page. you must use this as the published_date for all articles.\n",
    "    \"\"\"\n",
    "    # llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0, max_tokens=4096).with_structured_output(ArticleList)\n",
    "    llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0, max_tokens=8000).with_structured_output(ArticleList, include_raw=True)\n",
    "    \n",
    "    text = preprocess_pdf(path)\n",
    "    text_with_line_numbers = add_line_numbers(text)\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        output = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=text_with_line_numbers)])\n",
    "        articles = output['parsed'].articles\n",
    "        print_cost(cb)\n",
    "    print(f\"extracted {len(articles)} articles\")\n",
    "\n",
    "    docs = []\n",
    "    for article in articles:\n",
    "        article_text = '\\n'.join(text.split('\\n')[article.start_line:article.end_line])\n",
    "        metadata = {\n",
    "            \"source\": path.split(\"/\")[-1],\n",
    "            \"published_date\": article.published_date,\n",
    "            \"title\": article.title,\n",
    "            \"author\": article.author,\n",
    "            \"summary\": article.summary,\n",
    "            \"start_line\": article.start_line,\n",
    "            \"end_line\": article.end_line,\n",
    "            \"start_page\": article.start_page,\n",
    "            \"end_page\": article.end_page,\n",
    "        }\n",
    "        docs.append(Document(page_content=article_text, metadata=metadata))\n",
    "\n",
    "    return docs\n",
    "\n",
    "def save_docs_to_jsonl(array:list[Document], file_path:str)->None:\n",
    "    with open(file_path, 'w') as jsonl_file:\n",
    "        for doc in array:\n",
    "            jsonl_file.write(doc.model_dump_json() + '\\n')\n",
    "\n",
    "def load_docs_from_jsonl(file_path: str)->list[Document]:\n",
    "    array = []\n",
    "    with open(file_path, 'r') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            data = json.loads(line)\n",
    "            obj = Document(**data)\n",
    "            array.append(obj)\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tokens: 21150\n",
      "Completion Tokens: 1868\n",
      "Total Tokens: 23018\n",
      "Prompt Cost: $0.063450\n",
      "Completion Cost: $0.028020\n",
      "Total Cost: $0.091470\n",
      "extracted 12 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tokens: 22754\n",
      "Completion Tokens: 1915\n",
      "Total Tokens: 24669\n",
      "Prompt Cost: $0.068262\n",
      "Completion Cost: $0.028725\n",
      "Total Cost: $0.096987\n",
      "extracted 12 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tokens: 18992\n",
      "Completion Tokens: 1468\n",
      "Total Tokens: 20460\n",
      "Prompt Cost: $0.056976\n",
      "Completion Cost: $0.022020\n",
      "Total Cost: $0.078996\n",
      "extracted 9 articles\n"
     ]
    }
   ],
   "source": [
    "paths = glob(\"/Users/personal/projects/chitrapurgpt/documents/*.pdf\")\n",
    "\n",
    "articles = []\n",
    "for path in paths:\n",
    "    articles.extend(extract_articles(path))\n",
    "\n",
    "save_docs_to_jsonl(articles,'knowledge_base.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# save_docs_to_jsonl(articles,'knowledge_base.jsonl')\n",
    "\n",
    "articles=load_docs_from_jsonl('knowledge_base.jsonl')\n",
    "print(len(articles))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chitrapurgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
